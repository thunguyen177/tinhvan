{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep learnning in solving PDE.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S2-OvVEyVyeq","colab_type":"text"},"source":["# 1. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations\n","\n","[project codes](https://github.com/maziarraissi/PINNs)\n","\n","@article{raissi2019physics,\n","  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},\n","  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},\n","  journal={Journal of Computational Physics},\n","  volume={378},\n","  pages={686--707},\n","  year={2019},\n","  publisher={Elsevier}\n","}\n","\n","This paper concentrates in parametrized and nonlinear pde of the form\n","\n","$$u_t+\\mathcal{N}[u;\\lambda]=0,x\\in \\Omega, t\\in [0,T]$$\n","\n","where $u(t,x)$ denotes the latent solution, $\\mathcal{N}[.;\\lambda]$ is a nonlinear operator parametrized by $\\lambda$, $\\Omega$ is a subset of $\\mathbb{R}^D$.\n","\n","## Continuous time model\n","Let $f:= u_t+\\mathcal{N}[u;\\lambda]$ then the authors studies the shared parameters between the two neural networks $u(t,x)$ and $f(t,x)$ by minimizing the mean squared error loss\n","\n","$$MSE=MSE_u+MSE_f$$\n","\n","where \n","\n","$$MSE_u = \\frac{1}{N_u}\\sum_{i=1}^{N_u}|u(t_u^i,x_u^i)-u^i|^2,$$\n","\n","and \n","\n","$$MSE_f = \\frac{1}{N_u}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2.$$\n","\n","Here, $\\{t_u^i,x_u^i, u^i \\}_{i=1}^{N_u}$ denote the initial and boundary training data on $u(t,x)$ and $\\{t_f^i,x_f^i\\}_{i=1}^{N_f}$ specify the collocations points for $f(t,x).$ The loss $MSE_u$ corresponds to the initial and boundary data while $MSE_f$ enforces the structure imposed by the pde at a finite set of collocation points.\n","\n","Possible limitation: \n","\n","- Need to use a large number of collocation points $N_f$ in order to enforce physics-informed contraints in the entire spatio-temporal domain  --> dimensional curse issue for high-dimensional problems. This can be addressed to some extend using *sparse grid or Monte-Carlo samling*, using a more structured neural network representation leverageing the classical Runge-Kutta time-stepping schemes in the next section of the paper.\n","## Discrete time model\n","\n","The authors apply a q-stages to $u_t+\\mathcal{N}[u;\\lambda]=0,x\\in \\Omega, t\\in [0,T]$ and obtain\n","$$u^{n+c_i}=u^n-\\Delta t\\sum_{j=1}^qa_{ij}\\mathcal{ N} [u^{n+c_j}], i =1,...,q,$$\n","\n","$$u^{n+1}=u^n-\\Delta t\\sum_{j=1}^qb_{j}\\mathcal{ N} [u^{n+c_j}]$$\n","\n","where $u^{n+c_j}(x)=u(t^n+c_j\\Delta t,x)$ for $j=1,...,q$. So, $u^{n+c_j}(x)$ is the hidden state of the system at time $t^n+c_j\\Delta t$ for $j=1,...,q$. This system is equivalent to \n","\n","$$u^n=u_i^n,i=1,..,q,$$\n","\n","$$u^n=u^n_{q+1},$$\n","\n","where\n","\n","$$u_i^n := u^{n+c_i}+\\Delta t\\sum_{j=1}^qa_{ij}\\mathcal{ N} [u^{n+c_j}], i =1,...,q,$$\n","\n","$$u_{q+1}^n :=u^{n+1}+\\Delta t\\sum_{j=1}^qb_{j}\\mathcal{ N} [u^{n+c_j}].$$\n","\n","Then, they place a multi-output neural network prior on\n","\n","$$u^{n+c_1}(x),...,u^{n+c_q}(x), u^{n+1}(x)$$\n","\n","So, the input is $x$ and outputs are\n","\n","$$[u^n_1(x),...,u_q^n(x), u_{q+1}^n(x)]$$\n","\n","\n","*The paper also presents an approach to data-driven discovery pde similar to the 2 previous scheme for solving pde.*\n","\n","## Questions:\n","\n","- How deep/wide the network should be?\n","\n","- How much data is needed?\n","\n","- Effect of activation functions?\n","\n","- Are the MSE and sum of squared errors the appropriate functions?"]},{"cell_type":"markdown","metadata":{"id":"F-a_VS1cLPoz","colab_type":"text"},"source":["# DGM: A deep learning algorithm for solving partial differential equations\n","\n","@article{sirignano2018dgm,\n","  title={DGM: A deep learning algorithm for solving partial differential equations},\n","  author={Sirignano, Justin and Spiliopoulos, Konstantinos},\n","  journal={Journal of Computational Physics},\n","  volume={375},\n","  pages={1339--1364},\n","  year={2018},\n","  publisher={Elsevier}\n","}\n","\n","Consider the potentially nonlinear pde\n","\n","$$\\partial_t u(t,x) + \\mathcal{ L} u(t,x) = 0, \\;\\;\\; (t,x)\\in [0,T]\\times \\Omega$$\n","\n","$$u(0,x)=u_0(x),  $$\n","\n","$$ u(t,x)= g(t,x), \\;\\;\\; x\\in \\partial \\Omega$$\n","\n","The unknown solution $u(t,x)$ is usually approximate by $f(t,x)$ by minimizing the $L^2$ loss\n","\n","$$J(f)=||\\partial_t f+\\mathcal{ L}f||^2_{2, [0,T]\\times \\Omega} + ||f-g||^2_{2, [0,T]\\times \\partial \\Omega} + ||f(0,.)-u_0||^2_{2,\\Omega}.$$\n","\n","However, in this paper, the authors use a different parametrization. Instead of using $f(t,x)$, the unknown solution $u(t,x)$ is approximated using a deep neural network $f(t,x;\\theta ), \\theta\\in \\mathbb{R}^K, $ where $\\theta$  are parameters. The objective function is\n","\n"," $$J(f)=||\\partial_t f(t,x;\\theta )+\\mathcal{ L}f(t,x;\\theta )||^2_{ [0,T]\\times \\Omega,\\nu_1} + ||f(t,x;\\theta )-g(t,x )||^2_{ [0,T]\\times \\partial \\Omega,\\nu_2} + ||f(0,x;\\theta )-u_0(x)||^2_{\\Omega,\\nu_3}.$$\n"," \n"," and it is minimized using stochastic gradient descent. Each loop $n$ is trained with a random point $(t_n,x_n)$ is generated from $[0,T]\\times \\Omega$ w.r.t probability densities $\\nu_1$,  a random point $(\\tau_n,z_n)$ is generated from $[0,T]\\times \\partial \\Omega$ w.r.t probability densities $\\nu_2$,  a random point $w_n$ is generated from $\\Omega$ w.r.t probability densities $\\nu_3$.\n"," \n"," Problem: since stochastic gradient descent is used, stability poses an important questions.\n"]},{"cell_type":"markdown","metadata":{"id":"qB4GClXOgSrP","colab_type":"text"},"source":["# Data-driven discovery of partial differential equations\n"," [Code](https://github.com/snagcliffs/PDE-FIND)\n"," \n","@article{rudy2017data,\n","  title={Data-driven discovery of partial differential equations},\n","  author={Rudy, Samuel H and Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},\n","  journal={Science Advances},\n","  volume={3},\n","  number={4},\n","  pages={e1602614},\n","  year={2017},\n","  publisher={American Association for the Advancement of Science}\n","}\n","\n","In this paper, the author propose a method using ridge regression with hard thresholding to discover the governing pde of a given system. They parameterize the pde of general form\n","$$u_t = \\mathcal{ N}(u,u_x,u_{xx},...,x,\\mu)$$\n","The key assumption is that the function $\\mathcal{ N}$ consists of only a few terms so that the functional form is relatively sparse. This assumption itself limits the application of this method.\n","\n","The basic idea is to first collect data $U\\in \\mathbb{ C}^{n\\times m}$ of $m$ time points and $n$ spatial locations. We can also collect additional input such as the magnitude of complex data, a potential for Schodinger equation into $Q\\in  \\mathbb{ C}^{n\\times m}$. Next,  a library $\\Phi (U,Q)\\in C^{nm\\times D}$ of candidate terms $1, u, u^2, u_x,u_{xx},...$ is built, and are regarded as independent variable.  The problem turn into a least square problem, where we try to find the sparse vector $\\xi$ of coefficients by minimizing\n","$$||\\Phi(U,Q)\\xi-U_t||_2^2 + \\epsilon \\kappa (\\Phi(U,Q))||\\xi||_0$$\n","where $\\kappa(\\Phi)$ is the condition number of $\\Phi$, which indicates regularization against ill-posed problem.\n","\n","The advantages of this method are:\n","\n","1.   measurements can be collected in either a fixed or moving frame (Eulerian or Lagrangian)\n","2.   efficiently handle high-dimensional data through innovative sampling strategies\n","\n","Strategies: They use subsampled data. Subsampled data is created by randomly select a set spatial points and uniformly subsample in time. Although only a small fraction of the spatial points are in the sample, nearby points are needed to compute the derivative terms. The derivatives are computed using a small number of local points near each position via polynomial interpolation. This means we uses local information around each measurement in the sample.\n","\n","Drawbacks associated with sparse regression techniques: \n","- Derivatives are taken using polynomial interpolation, whose numerical approximations are ill-conditioned and unstable, due to truncation and round-off errors ([see more in Baydin](https://arxiv.org/abs/1502.05767)) . \n","Moreover, the evaluation of derivatives requires a large number of data, which makes the data amount of data used is much more than the size of subsampled data, which is used for regression. Thus, this method still requires far more data points than the number of functions in the library.  \n","\n","- The author assume that the chosen library is sufficiently rich to have a sparse representation of the underlying system. However, when the dynamics are unknown, this may not be true. In higher dimensional problems, the number of functions in the library increase exponentially. \n","\n","- It cannot estimate parameters of a pde involving the sines and cosines terms, even if we include these functions to the library.\n","\n","Possible redemies:\n","- The issue about numerical differentiation can be redemied by using Gaussian processes or neural networks. Then, the derivatives of the prior on $u$ can be compued by symbolic or automatic diferentiation. \n","- The second issue can be addressed by using neural network, which led to the next paper\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9PJeg_cMwtuJ","colab_type":"text"},"source":["# Deep hidden physics models: deep learning for nonlinear partial differential equations\n","[Code](https://github.com/maziarraissi/DeepHPMs)\n","\n","@article{raissi2018deep,\n","  title={Deep hidden physics models: Deep learning of nonlinear partial differential equations},\n","  author={Raissi, Maziar},\n","  journal={The Journal of Machine Learning Research},\n","  volume={19},\n","  number={1},\n","  pages={932--955},\n","  year={2018},\n","  publisher={JMLR. org}\n","}\n","\n","In this paper, the author propose a deep learning approach to discover the governing pde of a given system. They parameterize the pde of general form\n","$$u_t = \\mathcal{ N}(u,u_x,u_{xx},u_{xxx},...,x,\\mu)$$\n","They define a deep hidden physic model $f$ given by\n","$$f = u_t - \\mathcal{ N}(u,u_x,u_{xx},u_{xxx},...,x,\\mu)$$\n","The parameters of $u$ and $\\mathcal{ N}$ can be learned by minimizing the following sum of squared errors\n","$$SSE := \\sum_{i=1}^N (|u(t^i,x^i)-u^i|^2+|f(t^i,x^i)|^2),$$\n","where $\\{t^i,x^i,u^i\\}_{i=1}^N$ is training data.  \n","\n","The choice of activation functions, number of layers, number of neuron per layer still is an open question. See [Raissi](https://arxiv.org/abs/1711.10561), [Rassi II](https://arxiv.org/abs/1711.10566), [3](https://arxiv.org/abs/1801.01236) \n","\n","There is also a question of how common techniques such as batch normalization, dropout, $L^1,L^2$ regulirization could enhance the robustness of the algorithm. "]},{"cell_type":"code","metadata":{"id":"WLzBdOU6UGr6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}