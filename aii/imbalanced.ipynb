{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hsSzdJl7-ht"
   },
   "source": [
    "<h3> The imbalanced data problem </h3>\n",
    "<font color=\"#00bfff\"> \n",
    "The AI Institute's research campus' apartment resident site...\n",
    " \n",
    "All of the foreign students went outside to explore the city, leaving a bunch of Vietnamese here, inside the kitchen, finished their dinner.\n",
    "\n",
    " \n",
    "<img src = 'img/imbalanced_en.png'>\n",
    "  \n",
    "Duong blushed, \"Why should we care about such an <b>extreme event</b> instead of the things that lie at the <b>center region of normal distribution</b>? For example, who are gonna clear the table and wash the dishes?\" &#128580\n",
    " \n",
    "\"Well,  extreme events are usually interesting, and have many applications,\" Quang, the postdoc researcher said, as serious as if he was giving a lecture.\n",
    " \n",
    "\"For example?\" asked Duong.\n",
    " \n",
    "\"Like the bank can freeze a user's account when detecting irregular transactions, predicting storms, avalanches, tsunamis, epidemic, etc.,\" Quang said again.\n",
    " \n",
    "Before Duong could open her mouth again, Duy teased her, adding some words, \"And how some people can help us getting a girlfriend or boyfriend quickly.\"\n",
    " \n",
    "\"You're good at speaking, so I think that's easy for you. Just pick one and pursuit her!\" Thanh said.\n",
    " \n",
    "\"What if she runs away? Having a girlfriend is an optimization problem, \n",
    "of which the maximum of the loss function is a broken heart.\" Duy replied.\n",
    " \n",
    "\"Well, you should minimize the loss, not to maximize it,\" Duong said,\n",
    " \n",
    "\"In the past, I trained neural networks, and the loss exploded many times. \n",
    "So, I guess similar things may happen when I try to pursue a girl.\" Quang said, \n",
    "with a lot of honesty &#128580\n",
    " \n",
    "Everyone laughed again. &#128514\n",
    " \n",
    "\"Then you use <b>gradient clipping </b>, coddle her, give her gifts when she seems indifferent to you.\" Duong couldn't hold her laughter. \"By the way, it's not my turn to wash the dishes today. So, I'm gonna go back to my room and train my own network.\" Then, she left.\n",
    " \n",
    "\"I wonder, which loss function should we use for this problem?\" Duy asked.\n",
    " \n",
    "\"Binary cross-entropy, of course!\" Tuan smiled. \"Suppose the output from a girl $z$ is 1 if she likes you and 0 if she doesn't, and the probability that she likes you is $p$. Then the <b> binary cross-entropy </b> is defined as\n",
    "$$z\\log p + (1-z)\\log (1-p),$$\n",
    "which implies that if she likes you, then $z=1$ and loss is $log p$. If she doesn't, then $z=0$ and loss is $log (1-p)$.\n",
    " \n",
    "That's for one girl. If you decide to pursuit $n$ girls at the same time, then the loss is\n",
    "\n",
    "$$\\sum_{i=1}^n (z_i\\log p + (1-z_i)\\log (1-p)),\"$$\n",
    " \n",
    "\"Oh my god! Now I see who you really are. You pursuit $n$ girls at the same time!\" Quynh went ha ha ha ...\n",
    " \n",
    "\"Oh, no! For me, $n=1$ for sure. Yet, I was afraid that he couldn't have a wife when he is too old. So, I thought that he may be interested in pursuing $n$ girls at the same time.\" \n",
    " \n",
    "\"Who is older?\" Duy rolled his eyes. &#128548\n",
    " \n",
    "\"Of course, me!\" Tuan replied. Then, he tried to move on to another problem immediately. \n",
    "\"Let's talk about how to tackle this problem.\"\n",
    " \n",
    "\"Isn't this an imbalanced data problem?\" Quynh grinned.\n",
    " \n",
    "\"Yes, of course, I mostly succeeded in pursuing girls. \n",
    "So this is really a very imbalanced data issue.\" Tuan quickly replied before Quynh could \n",
    "continue by saying that he failed to pursuit girls most of the time &#128518.\n",
    "\n",
    "Still, Quynh did't let go easily, \"Is there any outlier, such as a widow or someone who is \n",
    "ten years older than you?\"\n",
    "\n",
    "\"No\"&#128548\n",
    " \n",
    "\"Maybe there is, but he already deleted it in his preprocessing step,\" Duy said. &#128518\n",
    " \n",
    "Everyone bursted into laughs again. \n",
    "\"This is a very small data problem. So, perhaps you should not drop a data point like that,\" Quynh said.\n",
    "\n",
    "\"Perhaps we should stop teasing him, before he <b>DropConnect</b> all of us!\" Quang said, but \n",
    "his words itselves is another tease. </font> \n",
    "\n",
    "<font color=\"#ff1493\"> Dropout: <i>At each training stage, individual nodes are either \"dropped out\" of the net with probability\n",
    " $1-p$ or kept with probability $p$, so that a reduced network is left; incoming and outgoing \n",
    "edges to a dropped-out node are also removed. Only the reduced network is trained on the data in\n",
    " that stage. The removed nodes are then reinserted into the network with their original weights.</i>\n",
    "(Wikipedia)\n",
    " \n",
    "In the training stages, the probability that a hidden node will be dropped is usually 0.5; \n",
    "for input nodes, this should be much lower, intuitively because information is directly lost \n",
    "when input nodes are ignored.</font> \n",
    "\n",
    "<font color=\"#ff1493\"> <b>DropConnect</b> DropConnect is the generalization of dropout in which\n",
    " each connection, rather than each output unit, can be dropped with probability $1-p$. \n",
    "Each unit thus receives input from a random subset of units in the previous layer.\n",
    "</font>\n",
    " <font color=\"#00bfff\"> \n",
    "\"So, are you gonna generate synthesis data using SMOTE or ADASYN, or Nearmiss to generate some \n",
    "synthetic girlfriends?\" asked Quynh &#128527. </font> \n",
    " \n",
    "\n",
    "<ul>\n",
    "\n",
    "<li> <font color=\"#ff1493\"> <b> <a href='https://arxiv.org/pdf/1106.1813.pdf'>SMOTE</a> \n",
    "(Synthetic Minority Over-sampling Technique):</b> rather than creating new ra\n",
    "ndom minority samples from existing minority classes, we over-sample the minorit\n",
    "y class by creating “synthetic” samples.\n",
    "\n",
    "<li>  <b> ADASYN</b>: \"The essential idea of ADASYN is to use a weighted\n",
    "distribution for different minority class examples according to\n",
    "their level of difficulty in learning, where more synthetic data\n",
    "is generated for minority class examples that are harder to\n",
    "learn compared to those minority examples that are easier to\n",
    "learn. As a result, the ADASYN approach improves learning\n",
    "with respect to the data distributions in two ways: (1) reducing\n",
    "the bias introduced by the class imbalance, and (2) adaptively\n",
    "shifting the classification decision boundary toward the difficult\n",
    "examples.(source: <a href='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.942&rep=rep1&type=pdf'>ADASYN: \n",
    "Adaptive Synthetic Sampling Approach for Imbalanced\n",
    "Learning</a>  \n",
    "\n",
    "<li>Nearmiss methods aim to under-sample the points from the majority class that is neccessary to distinguish between other classes. \n",
    "It's based on k-nearest neighbor clustering. \n",
    "See more in <a href='https://arxiv.org/pdf/1608.06048.pdf'>Survey of resampling techniques for improving\n",
    "classification performance in unbalanced datasets</a> \n",
    "</ul>\n",
    "</font>\n",
    " \n",
    "<font color=\"#00bfff\"> \n",
    "Tuan understood what Quynh really means by <b><i>synthetic girlfriend</b>. Yet, he ignored it and continued. \"Those techniques are\n",
    "yesterday. I'll use customized loss function, \"a specialist for imbalanced data\" instead!\"\n",
    "\n",
    "\"What is that?\" Duy asked.\n",
    "\n",
    "\"Not gonna tell you!\" Tuan asked and then walked up stair.\n",
    "\n",
    "Several other started to leave as well. Only three people left: two boys that washed the dish, and one girl who was looking at them.\n",
    " \n",
    "One boy asked, \"Is it look that interesting to you about the way we wash the dishes?\"\n",
    " \n",
    "\"Nope. I just want to know how many dishes you guys gonna break to report to the manager tommorrow!!!\" \n",
    "</font> \n",
    "\n",
    " \n",
    " \n",
    "\n",
    "You may interest in:\n",
    " \n",
    "<ul>\n",
    "<li><a href='https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?fbclid=IwAR2GYZmJ6aBWpoFxQkSi7zlUULUjB1mkPY_-uFDbtBjN0wRFiEr4po79ixY'>\n",
    "Classification on imbalanced data (tutorial on tensorflow.org)\n",
    "<li> Cost-Sensitive Learning of Deep Feature\n",
    "Representations from Imbalanced Data: a paper by S. H. Khan, M. Hayat, M. Bennamoun, F. Sohel and R. Togne\n",
    "<li>\tTraining Deep Neural Networks on Imbalanced Data Sets:  a paper that concentrate on <b> binary classification </b> by Shoujin Wang \u0014\n",
    ", Wei Liu\n",
    ", Jia Wu \n",
    ", Longbing Cao \n",
    ", Qinxue Meng \n",
    ", Paul J. Kennedy \n",
    "</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "aii_imbalanced_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
