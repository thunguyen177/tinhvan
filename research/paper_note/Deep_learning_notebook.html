<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <title>Deep learning notebook</title>
    <link rel="stylesheet" href="../css/style.css"/>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
</head>
<body>
   <header>
        <div class="container">
            <h1> Ellie Nguyen </h1>
        </div>
    </header>

  <nav id="navbar">
      <div class="containerblue">
        <ul>
          <li><a href="../index.html"> Home</a> </li>
          <li> <a href="../ML%20and%20stat/index.html"> Data science</a> </li>
	  <li> <a href="../paper_note/index.html"> Deep learning paper notes</a> </li>
          <li> <a href="../Fantistics/index.html">Statistics is fun</a> </li>
          <li> <a href="../programming/index.html">Programming</a> </li>
        </ul>
      </div>
      <div class="containerpink">
        <ul>
	  <li> <a href="../vnm%20cul_his/index.html"> Vietnamese culture</a> </li>
          <li><a href="../Toi%20sang%20tac/index.html">Tác phẩm</a>
	  <li><a href="../thong_ke_vui/index.html">Vui học thống kê</a>
          <li><a href="../goc%20du%20hoc/index.html">Du học và ngoại ngữ</a> </li>
        </ul>
      </div>
  </nav>
    <div class = "searchbar">
        <script>
              (function() {
                var cx = '000342376851758299742:vxnvik3_5yc';
                var gcse = document.createElement('script');
                gcse.type = 'text/javascript';
                gcse.async = true;
                gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(gcse, s);
              })();
        </script>
        <gcse:search></gcse:search>
    </div>

<h3> Index </h3>
  	<a href="#MLEloss">Loss function</a><br>
	<a href="#sigout">Softmax</a><br>
	<a href="#mix">Gaussian mixture output</a><br>
	<a href="#depth">Depth and activation function </a><br>
	<a href='#regu'>Regularization</a> <br> 
	<a href='#noise'>Noise</a> <br> 




					<hr color = "#F333FF" width="400px"> 
<h3 id="MLEloss"> Maximum likelihood loss function</h3>
Many times, the loss function for a deep learning network is a the negative log-likelihood

$$ J(\theta)=-\mathbb{E}_{x,y\sim \hat{p}_{data}} \log_{model} p(y|x)$$
This provide a convenient framework for deep learning model. Also, in deep learning, we want to try to avoid the problem of saturation, and the negative log-likelohood
helps ameliorate this problem. 
<br> 
Solving the optimization problem \(f^*(x)=\arg\min_{f}\mathbb{E}_{x,y\sim p_{data}}||y-f(x)||^2\) yields \(f^*(x)=\mathbb{E}_{y\sim p_{data}(y|x)}[y]\),
 i.e., if we can train  on infinitely many samples, the output from minimizing the mean squared error is a function that predicts the mean of \(y\) for each \(x\). 
<br> 

Another result is that when the mean absolute error function is used, 
\(f^*(x)=\arg\min_{f}\mathbb{E}_{x,y\sim p_{data}}||y-f(x)||_1\) yields a function that predicts the <font color="#00bfff"> median </font> value of \(y\) for each \(x\).
<br> 
However, mean squared error, mean absolute error often lead to poor results when used with gradient-based optimization. Hence, the cross-entropy loss function is more 
popular. The form of the cross-entropy loss function is usually determined by the the choice of how to represent the output. <br> 

<hr color = "#F333FF" width="400px"> 
<h3 id="sigout">Softmax </h3>
Let \( z_i=\log \widetilde{P}(y=i|x)\) then \(softmax(z)_i=\frac{\exp(z_i)}{\sum_j \exp(z_j)}\). By definition, we can see when the softmax function saturates 
to 0 or 1. <br> 

Many objective functions other than the log-likehood do not work well with the softmax function, as they do not use a 'log' term to undo the 'exp' terms, causing
the gradient to vanish do to saturation. Though, when the input magnitude is extreme, the output of the sigmoid function may saturate. Then, many softmax-based 
cost function also saturate, unless they can inveert the saturating activating function. <br>

Note that \(softmax(z)=softmax(z+c)\). So, we can use a numerically stable variant of the softmax 
$$softmax(z)=softmax(z-\max_i z_i)$$

							<hr color = "#F333FF" width="400px"> 
						<h3 id="mix">Gaussian mixture output</h3>
A Gaussian mixture output with n components is defined by
$$p(y|x)=\sum_{i=1}^np(c=i|x)\mathcal{ N} (y; \mu^{(i)}(x),\Sigma^{(i)}(x))$$
and a neural networs with Gaussian mixture as their output is called a <font color="red"> mixture density network. </font> <br>
Note that when learning a single Gaussian component, we usually use a diagonal matrix for \(\Sigma^{(i)}\) to avoid computing determinants.  <br> 
Gradient-based optimizatio for conditional Gaussian mixtures on the output of the neural networks can be numerically unstable when the variance gets to small for a 
particular example. A solution to this is to clip gradients or to scale the gradient heirostocally.

						<hr color = "#F333FF" width="400px"> 
						<h3 id='depth'>Depth and activation function </h3>
Deeper networks ae often able to use far fewer units per layer as well as muc less parameters while being able to generalize to test set. Though, they also tend to
be harder to optimize. <br> 
 When the network is very small, sigmoid activation function is better than ReLu. In <a href='http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf'>
What is the Best Multi-Stage Architecture for Object Recognition?</a> illustrates that it's more important to use ReLu rather than learning the weights of 
the hidden layers, i.e., random weights are sufficient to propagate important information through a rectified linear network.


						<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>
How it works: trading increased bias for reduced variance. An effective regularizer will reduce variance a significant amount while not increase the bias overly. <br> 
Sometimes, penalty method and Augmented Lagrangian method can be used.
If we have a loss function \(J(\theta,X,y)\) then a regularized cost function by a parameter norm penalty is 
$$\widetilde{J}(\theta,X,y)=J(\theta,X,y)+\alpha \Omega(\theta).$$
A nice result from <i>An Introduction to Optimization by Chong & Zak </i>: <br> 
Suppose we want to minimize a continuous function \(f(x)\) subject to \(g_1(x)\le 0,...,g_p(x)\le 0\) by minimizing the following unconstrained problem
$$q(\gamma_k,x)=f(x)+\gamma_kP(x)$$
for each \(k=1,2,...\), and \(\gamma_k\in R\) is a given positive constant, \(\gamma_k\rightarrow \infty\) as \(k \rightarrow \infty\). <br> 
Let \(x_k\) be a minimizer of \(q(\gamma_k,x)\). Then the limit of any convergent subsequence of \(\{x_k\}\) is a solution to the constrained optimization problem.<br>
Note that Augmented Lagrangian method is more stable than the penalty approach. <br> 
However, sometimes penalty approach is not preferred as it can cause nonconvex optimization procedures to get stuck in local minima due to small \(\theta\). 
In such cases, explicit constraints with reprojection can be great tools as they impose on the optimization procedure some stability. <br>

<a href='https://arxiv.org/abs/1207.0580'>Hinton
et al., Improving neural networks by preventing co-adaptation of feature detectors </a>  suggests the followings
<ul>
	<li> using constraints combined with a high learning rate to explore the parameter space rapidly while maintaining some stabiity.<br> 
	<li> constraining the norm of each <i>column </i> of the weight matrix of a neural network layer rather than using the Frobenious norm with the whole weight 
matrix. This prevents any one hidden unit from having very high weights. This is always implemented as an explicit constraint with reprojection.
</ul>

						<hr color = "#F333FF" width="400px"> 
						<h3 id='noise'>Noise</h3>
It's wellknown that neural networks are not very robust to noise (<a href='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.170.1765&rep=rep1&type=pdf'>
Deep networks for robust visual recognition</a>). However, for many classification/regression tasks, it should work if small random noise is added to the input. 
To improve robustness of neural networks to noises, we can train them by adding to the inputs random noises which has infinitesimal variance. <a href='https://arxiv.org/pdf/1406.1831.pdf'>
analyzing noise in autoencoders and deep networks</a> shows that this can be highly effective if the magnitude of the noise is carefully tuned.
In general, noise injection can be far more powerful than shrinking the parameters, especially when noises are added to the hidden units. <br>

In recurrent neural networks, noise can also be added to the weights. This can be interpreted as a regulization scheme to encourage the stability of the function to be 
learned. <br> 
Sometimes, noise is also injected to the output to account for the mistakes in the labels. Ex: <font color="red"> label smoothing</font> regularizes a model based on
a k-output value softmax by replacing 0, 1 targets with \(\frac{\epsilon }{k-1}, 1-\epsilon\) targets. Cross-entropy loss can be used with these soft targets.

	<!--					<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>



						<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>



						<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>



						<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>



						<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>



						<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>


						<hr color = "#F333FF" width="400px"> 
						<h3 id='regu'>Regularization </h3>
-->

    <!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5cf9633718532342"></script>
    <div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v3.3"></script>
<div class="fb-comments" data-href="https://ellienguyen.style/" data-width="" data-numposts="5"></div>
    <footer id="main-footer">
        <hr color = "black" width="400px">
            		Copyright &copy; 2019 Ellie Nguyen. All rights reserved.
	</footer>
</body>
</html>